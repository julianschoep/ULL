{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import distributions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 10000\n",
    "NUM_EPOCHS = 100\n",
    "vocab_size_limit = 10000\n",
    "\n",
    "filename = \"wa/dev.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"english\") as stopwords:\n",
    "    english_stop_words = stopwords.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data, clean them, make a vocabulary and create dictionaries\n",
    "with open(\"english\") as stopwords:\n",
    "    english_stop_words = stopwords.read().split()\n",
    "\n",
    "corpus = []\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "with open(filename) as f:\n",
    "    for sentence in f:\n",
    "        clean_sentence = sentence.lower()\n",
    "        clean_sentence = sentence.translate(translator)\n",
    "        clean_word = clean_sentence.split()\n",
    "        clean_word = [w.lower() for w in clean_word]\n",
    "        corpus.append(clean_word)\n",
    "\n",
    "print(len(corpus))\n",
    "flat_list = [item for sublist in corpus for item in sublist]\n",
    "corpus_set = set(flat_list)\n",
    "vocabulary = defaultdict(lambda: 0)\n",
    "text = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        if word not in english_stop_words:\n",
    "            vocabulary[word] +=1\n",
    "            \n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "word2idx[\"UNK\"] = len(word2idx)\n",
    "idx2word[len(word2idx)] = \"UNK\"\n",
    "\n",
    "for sentence in corpus:\n",
    "    text.append([word2idx[word]if word in word2idx.keys() else word2idx[\"UNK\"] for word in sentence])\n",
    "\n",
    "vocab_size = len(vocabulary)    \n",
    "# make the word pairs and return dictionary\n",
    "pairs = {}\n",
    "center = []\n",
    "context= []\n",
    "for sentence in corpus:\n",
    "    for central_word in range(len(sentence)):\n",
    "        pairs[sentence[central_word]] = []\n",
    "        for current_window in range(-WINDOW_SIZE, WINDOW_SIZE +1):\n",
    "            context_word = central_word + current_window            \n",
    "            if (context_word <=0 or context_word >= len(sentence) or context_word==central_word):\n",
    "                continue\n",
    "            if sentence[context_word] not in vocabulary:\n",
    "                context.append(word2idx[\"UNK\"])\n",
    "            else:\n",
    "                context.append(word2idx[sentence[context_word]])\n",
    "            if sentence[central_word] not in vocabulary:\n",
    "                center.append(word2idx[\"UNK\"])\n",
    "            else:\n",
    "                center.append(word2idx[sentence[central_word]])\n",
    "                \n",
    "list_of_center = []\n",
    "list_of_context = []\n",
    "center_update = []\n",
    "context_update = []\n",
    "for center, context in zip(center,context):\n",
    "    if center != 10001:\n",
    "        list_of_center.append(center_update)\n",
    "        list_of_context.append(context_update)\n",
    "        center = 10001\n",
    "        center_update = []\n",
    "        context_update = []\n",
    "    \n",
    "    center_update.append(center)\n",
    "    context_update.append(context)\n",
    "list_of_center.pop(0)\n",
    "list_of_context.pop(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bayesian_skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(bayesian_skipgram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.mu_prior = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.sigma_prior = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.M = nn.Linear(embed_dim*2, embed_dim)\n",
    "        self.lambda_mu = nn.Linear(embed_dim, embed_dim)\n",
    "        self.lambda_sigma = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sofmax = nn.Softmax()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, word_idx, context_idx):\n",
    "        batch_size = BATCH_SIZE\n",
    "        #forward step \n",
    "        word_emb = self.embedding(word_idx)               \n",
    "        context_emb = self.embedding(context_idx)        \n",
    "        WC = torch.cat((word_emb, context_emb), dim=2)   \n",
    "        \n",
    "        m = self.relu(self.M(WC))\n",
    "        h = torch.sum(m, dim=1)\n",
    "        \n",
    "        mu = self.lambda_mu(h)\n",
    "        sigma = self.softplus(self.lambda_sigma(h))\n",
    "        \n",
    "        eps = distributions.MultivariateNormal(torch.zeros(self.embed_dim), torch.eye(self.embed_dim)).sample()\n",
    "        z = mu + sigma * eps \n",
    "        \n",
    "        out = self.out(z)\n",
    "        \n",
    "        likelihood_terms = torch.zeros(BATCH_SIZE)\n",
    "        KL_div_terms = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        for i, contexts in enumerate(context_idx):  \n",
    "            likelihood = 0\n",
    "            for idx in contexts:\n",
    "                likelihood += torch.log(f_i[i, idx] +1e-8)\n",
    "            likelihood_terms[i] = likelihood\n",
    "\n",
    "            KL =  self.KL_div(mu_prior[i], sigma_prior[i],  mu[i],  sigma[i] )\n",
    "            KL_div_terms[i] = KL\n",
    "\n",
    "\n",
    "        total_loss = torch.mean(KL_div_terms) - torch.mean(likelihood_terms)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_div(self,  mu_p, sigma_p, mu, sigma):\n",
    "    div = torch.log(sigma_p + 1e-8) - torch.log(sigma+1e-8) + (sigma**2 + (mu - mu_p)**2) / (2*sigma_p**2) - 0.5\n",
    "    return div.sum()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses= []\n",
    "epoch_loss = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "iterations = len(list_of_context)\n",
    "\n",
    "\n",
    "\n",
    "batch = 0\n",
    "\n",
    "\n",
    "for center, context in zip(list_of_center,list_of_context):\n",
    "    batch_start = time.time()\n",
    "\n",
    "    \n",
    "    center = torch.LongTensor(center).cuda()\n",
    "    context = torch.LongTensor(context).cuda()\n",
    "    center = Variable(center).cuda()\n",
    "    context = Variable(context).cuda()\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    out, mu, sigma = bsg_model(center,context)\n",
    "    cat_out = out.repeat(len(context),1) \n",
    "\n",
    "    CE_loss = loss_function(cat_out, context)\n",
    "    prior = torch.distributions.multivariate_normal.MultivariateNormal(torch.ones(embedding_dims).cuda(), torch.eye(embedding_dims).cuda())\n",
    "    posterior = torch.distributions.multivariate_normal.MultivariateNormal(u, torch.diag(s))\n",
    "    KL_div_terms = torch.distributions.kl.kl_divergence(posterior, prior).sum()\n",
    "    loss = -CE_loss + kl \n",
    "    epoch_loss += loss.data.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_time = time.time() - batch_start \n",
    "    batch += 1\n",
    "\n",
    "epoch_loss /= iterations\n",
    "\n",
    "epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
