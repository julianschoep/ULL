{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Word Similarity </b> <br>\n",
    "Your next step is to evaluate the three kinds of embeddings in the word similarity\n",
    "task. The goal of this task is to compute similarity of two words and\n",
    "evaluate the model-produced similarity against human judgements. Download\n",
    "two commonly-used word similarity datasets:<br>\n",
    "• SimLex: https://www.cl.cam.ac.uk/˜fh295/simlex.html<br>\n",
    "• MEN: https://staff.fnwi.uva.nl/e.bruni/MEN<br>\n",
    "Compute cosine similarity between the words using the three models. Evaluate\n",
    "the model-produced similarities against human judgements in terms of\n",
    "Pearson and Spearman correlation coefficients. <br>\n",
    "Compare the performance of\n",
    "the three models on this task. <br>\n",
    "Analyze the data qualitatively and report what\n",
    "are the differences in the kind of similarity captured by the three models.<br> We\n",
    "are interested to see both quantitative results and qualitative analysis in your\n",
    "report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_SimLex():\n",
    "    # Read in the SimLex dataset\n",
    "    SimLex_file = open('SimLex-999/SimLex-999.txt','r')\n",
    "    SimLex_set = {}\n",
    "    next(SimLex_file)\n",
    "    for line in SimLex_file:\n",
    "        pair_data = [x.strip() for x in line.split('\\t')]\n",
    "        word1 = pair_data[0]\n",
    "        word2 = pair_data[1]\n",
    "        similarity_score = pair_data[4]\n",
    "        SimLex_set[(word1, word2)] = {'human':similarity_score}\n",
    "    return SimLex_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the MEN dataset\n",
    "def read_in_MENS():\n",
    "    MEN_file = open('MEN/MEN_dataset_natural_form_full','r')\n",
    "    MEN_set = {}\n",
    "    for line in MEN_file:\n",
    "        pair_data = [x.strip() for x in line.split(' ')]\n",
    "        word1 = pair_data[0]\n",
    "        word2 = pair_data[1]\n",
    "        similarity_score = pair_data[2]\n",
    "        MEN_set[(word1, word2)] = {'human':similarity_score}\n",
    "    return MEN_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to reduce the size of the word embedding files,\n",
    "#  I will delete all non-occuring words from the relevant model and\n",
    "# store the word embeddings in a defaultdict for quick access.\n",
    "def get_occuring_words(MEN_set, SimLex_set):\n",
    "    occuring_words = []\n",
    "    for pair in list(MEN_set.keys())+list(SimLex_set.keys()):\n",
    "        word1 = pair[0]\n",
    "        word2 = pair[1]\n",
    "        if word1 not in occuring_words:\n",
    "            occuring_words.append(word1)\n",
    "        if word2 not in occuring_words:\n",
    "            occuring_words.append(word2)\n",
    "    return occuring_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the word vectors for a given word model\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# switch between 'bow5.words', 'bow2.words' and 'deps.words'\n",
    "def get_embeddings_similarity(filename, occuring_words):\n",
    "    # Returns a dictionary containing words as keys and word vectors\n",
    "    #  as values\n",
    "    if filename+'_small' in os.listdir():\n",
    "        f = open(filename+'_small')\n",
    "        using_small = True\n",
    "    else:\n",
    "        f = open(filename,'r')\n",
    "        d = open(filename+'_small','w')\n",
    "    word_embeddings = defaultdict(list)\n",
    "    i =0\n",
    "    s = time.time()\n",
    "    for line in f:\n",
    "        entry = [x.strip() for x in line.split(' ')]\n",
    "        word = entry[0]\n",
    "        vector = entry[1:]\n",
    "        if word in occuring_words:\n",
    "            word_embeddings[word] = [np.float(x) for x in vector]\n",
    "            if not using_small:\n",
    "                d.write(line)\n",
    "    if not using_small:\n",
    "        d.close()\n",
    "    f.close()\n",
    "    e = time.time()\n",
    "    print('It took {} seconds to read in dataset {}'.format(e-s,filename))\n",
    "    return word_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def cosine_similarity(v1, v2,n1, n2):\n",
    "    # Returns a value between -1 and 1, 1 meaning exactly same\n",
    "    #  and -1 meaning exactly opposite.\n",
    "    assert len(v1) == len(v2)\n",
    "    numerator = sum([v1[i]*v2[i] for i in range(len(v1))])\n",
    "    denominator = np.sqrt(sum([x**2 for x in v1])) \\\n",
    "                * np.sqrt(sum([x**2 for x in v2]))\n",
    "    if denominator == 0:\n",
    "        print('division by zero with words:')\n",
    "        print(n1)\n",
    "        print(n2)\n",
    "    return (numerator/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_cosine_similarity(dataset, word_embeddings):\n",
    "    # Computes the cosine similarity per word pair and adds\n",
    "    #  it to the pair dictionary entry.\n",
    "    for pair in list(dataset.keys()):\n",
    "        v1, v2 = word_embeddings[pair[0]], word_embeddings[pair[1]]\n",
    "        if len(v1) != len(v2):\n",
    "            if len(v1) == 0 or len(v2) == 0:\n",
    "                print('Removing {} and {} from dataset'.format(pair[0],pair[1]))\n",
    "                print('No word embedding found for one of them')\n",
    "                del dataset[pair]\n",
    "            else:\n",
    "                print(\"Something weird happened with:\")\n",
    "                print(pair[0], pair[1])\n",
    "        else:\n",
    "            cos_sim = cosine_similarity(v1,v2,pair[0],pair[1])\n",
    "            dataset[pair]['cos_sim'] = cos_sim\n",
    "    return dataset\n",
    "\n",
    "def get_just_data(dataset):\n",
    "    # Returns the human judgements and cosine similarities \n",
    "    # as a nx2 numpy array for easy data anaylsis\n",
    "    n = len(dataset)\n",
    "    result = np.zeros((2,n))\n",
    "    for i,pair in enumerate(list(dataset.keys())):\n",
    "        result[0,i] = dataset[pair]['human']\n",
    "        result[1,i] = dataset[pair]['cos_sim']\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_measures(filename):\n",
    "    s = time.time()\n",
    "    MEN_set = read_in_MENS()\n",
    "    SimLex_set = read_in_SimLex()\n",
    "    occuring_words = get_occuring_words(MEN_set, SimLex_set)\n",
    "    word_embeddings = get_embeddings_similarity(filename,occuring_words)\n",
    "    MEN_set = add_cosine_similarity(MEN_set, word_embeddings)\n",
    "    SimLex_set = add_cosine_similarity(SimLex_set, word_embeddings)\n",
    "    MEN_data = get_just_data(MEN_set)\n",
    "    SimLex_data = get_just_data(SimLex_set)\n",
    "    e = time.time()\n",
    "    print('Getting similarity measures on {} took {} seconds'.format(filename,(e-s)))\n",
    "    return MEN_data, SimLex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the three datasets\n",
    "MEN_k2, Sim_k2 = get_similarity_measures('bow2.words')\n",
    "MEN_k5, Sim_k5 = get_similarity_measures('bow5.words')\n",
    "MEN_deps, Sim_deps = get_similarity_measures('deps.words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "\n",
    "def precision(data,k):\n",
    "    n = len(data[0,:])\n",
    "    d = data[:,data[0,:]]\n",
    "    return None\n",
    "data = MEN_deps\n",
    "d = data[:,data[0,:].argsort()]\n",
    "rg_x = ss.rankdata(d[0,:],'dense')\n",
    "rg_y = ss.rankdata(d[1,:],'dense')\n",
    "\n",
    "n = len(data[0,:])\n",
    "for i in range(n):\n",
    "    print(d[:,i],rg_x[i],rg_y[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "\n",
    "def calc_Pearson_correlation(data):\n",
    "    # data[0,:] are the human observations\n",
    "    # data[1,:] are the computed cosine similarities\n",
    "    assert data.shape[0] == 2, 'Data must have two measures per variable.'\n",
    "    cov_matrix = np.cov(data)\n",
    "    assert cov_matrix[1,0] == cov_matrix[0,1],'These should be same...'\n",
    "    cov_hum_cos = cov_matrix[1,0]\n",
    "    std_hum = np.std(data[0,:])\n",
    "    std_cos = np.std(data[1,:])\n",
    "    rho = cov_hum_cos/(std_hum*std_cos)\n",
    "    return rho\n",
    "\n",
    "def calc_Spearman_correlation(data):\n",
    "    # Source https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\n",
    "    n = len(data[0,:])\n",
    "    x = data[0,:]\n",
    "    y = data[1,:]\n",
    "    rg_x = ss.rankdata(x)\n",
    "    rg_y = ss.rankdata(y)\n",
    "    rg_data = np.array([rg_x,rg_y])\n",
    "    cov_matrix = np.cov(rg_data)\n",
    "    assert cov_matrix[1,0] == cov_matrix[0,1],'These should be same...'\n",
    "    cov_rg_xy = cov_matrix[1,0]\n",
    "    std_rg_x = np.std(rg_x)\n",
    "    std_rg_y = np.std(rg_y)\n",
    "    rho_rg = cov_rg_xy/(std_rg_x*std_rg_y)\n",
    "    return rho_rg\n",
    "\n",
    "def pearson_bootstrap_test(data, dataset_name, model_name):\n",
    "    n = len(data[0,:])\n",
    "    no_samples = 10000\n",
    "    sample_size = int(np.floor(n/20))\n",
    "    rho_full = calc_Pearson_correlation(data)\n",
    "    rho_samples = []\n",
    "    for i in range(no_samples):\n",
    "        rand_idx = np.random.choice(range(n),n,replace=True,)\n",
    "        sample_data = data[:,rand_idx]\n",
    "        rho_sample = calc_Pearson_correlation(sample_data)\n",
    "        rho_samples.append(rho_sample)\n",
    "    rho_samples.sort()\n",
    "    rho_lower = rho_samples[int(no_samples*.025)]\n",
    "    rho_upper = rho_samples[int(no_samples*.975)]\n",
    "    plt.title('Pearson Correlation Coefficient Boostrap test \\n Dataset: {} \\n Model: {}'.format(dataset_name, model_name))\n",
    "    counts,b,p = plt.hist(rho_samples,bins=100)\n",
    "    label_height = max(counts)\n",
    "    plt.xlabel(r'$\\rho$')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(x=rho_full,c='red')\n",
    "    plt.text(rho_full,label_height,r'$\\rho_{pop}$'+\": {0:.{1}f}\".format(rho_full,3))\n",
    "    plt.axvline(x=rho_lower,c='red')\n",
    "    plt.text(rho_lower,label_height,r'$\\rho_{lo}$'+\": {0:.{1}f}\".format(rho_lower,3))\n",
    "    plt.axvline(x=rho_upper,c='red')\n",
    "    plt.text(rho_upper,label_height,r'$\\rho_{up}$'+\": {0:.{1}f}\".format(rho_upper,3))\n",
    "    plt.show()\n",
    "    return rho_samples\n",
    "\n",
    "\n",
    "#plt.scatter(Sim_k5[1,:],Sim_k5[0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation bootstrap tests\n",
    "\n",
    "#pearson_bootstrap_test(MEN_k2,'MEN','BOW with k=2')\n",
    "#pearson_bootstrap_test(MEN_k5,'MEN','BOW with k=5')\n",
    "#pearson_bootstrap_test(MEN_deps,'MEN','Dependency based')\n",
    "#pearson_bootstrap_test(Sim_k2,'SimLex','BOW with k=2')\n",
    "#pearson_bootstrap_test(Sim_k5,'SimLex','BOW with k=5')\n",
    "#pearson_bootstrap_test(Sim_deps,'SimLex','Dependency based')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_Spearman_correlation(MEN_k2))\n",
    "print(calc_Spearman_correlation(MEN_k5))\n",
    "print(calc_Spearman_correlation(MEN_deps))\n",
    "print(calc_Spearman_correlation(Sim_k2))\n",
    "print(calc_Spearman_correlation(Sim_k5))\n",
    "print(calc_Spearman_correlation(Sim_deps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 22.93110704421997 seconds to read in dataset bow5.words\n"
     ]
    }
   ],
   "source": [
    "# switch between 'bow5.words', 'bow2.words' and 'deps.words'\n",
    "def get_embeddings(filename):\n",
    "    # Returns a dictionary containing words as keys and word vectors\n",
    "    #  as values\n",
    "    \n",
    "    f = open(filename,'r')\n",
    "    word_embeddings = defaultdict(list)\n",
    "    s = time.time()\n",
    "    for line in f:\n",
    "        entry = [x.strip() for x in line.split(' ')]\n",
    "        word = entry[0]\n",
    "        vector = entry[1:]\n",
    "        \n",
    "        word_embeddings[word] = [np.float(x) for x in vector]\n",
    "    f.close()\n",
    "    e = time.time()\n",
    "    print('It took {} seconds to read in dataset {}'.format(e-s,filename))\n",
    "    return word_embeddings\n",
    "\n",
    "bow5_embeddings = get_embeddings('bow5.words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183870, 301)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "i = 0\n",
    "n = len(list(bow5_embeddings.keys()))\n",
    "X = np.zeros((n,300))\n",
    "y = []\n",
    "for i,word in enumerate(list(bow5_embeddings.keys())):\n",
    "    word_vector = np.array(bow5_embeddings[word])\n",
    "    X[i,:] = word_vector\n",
    "    y.append(word)\n",
    "df = pd.DataFrame(X)\n",
    "df['label'] = y\n",
    "df['label'] = df['label'].apply(lambda i: str(i))\n",
    "print(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183870\n"
     ]
    }
   ],
   "source": [
    "rndperm = np.random.permutation(df.shape[0]) # Random indices\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(df[range(n)].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
