{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy Task"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate the three models in the word analogy task. Given an analogy\n",
    "a : a∗ :: b :?, the task is to find b∗.\n",
    "Use this link to obtain the Google Word Analogy test set:\n",
    "• https://aclweb.org/aclwiki/Google analogy test set (State of the art)\n",
    "To solve the word analogy tasks with the given word embeddings, you should\n",
    "use the vector offset method based on the cosine distance. This means, to answer\n",
    "the question a : a∗ :: b : b∗, where b∗ is unknown, to find the embedding of b∗,\n",
    "we simply first compute the offset vector: v = a ∗ −a, next compute the vector\n",
    "representation of the word we expect: b∗ = b + v. Most probably, there is no\n",
    "word with the exact same vector we compute this way. Hence, retrieve the word\n",
    "which its vector representation is closer to predicted vector based on cosine\n",
    "distance. Note that, in this task it is important to normalize the word vectors\n",
    "to unit form.\n",
    "You need to report the accuracy and MRR (Mean Reciprocal Rank) of each\n",
    "of the word embedding models on this task. The accuracy is the percentage of\n",
    "the correctly answered questions from the word analogy question set.\n",
    "Again, in addition to the quantitative evaluation, conduct a qualitative analysis\n",
    "of the differences between the three models. Include both in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
=======
   "cell_type": "code",
   "execution_count": 86,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
<<<<<<< HEAD
    "import heapq\n",
    "from scipy.spatial.distance import cosine\n"
=======
    "import heapq"
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 2,
   "metadata": {
    "collapsed": true
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2,n1, n2):\n",
    "    # Returns a value between -1 and 1, 1 meaning exactly same\n",
    "    #  and -1 meaning exactly opposite.\n",
    "    assert len(v1) == len(v2)\n",
    "    numerator = sum([v1[i]*v2[i] for i in range(len(v1))])\n",
    "    denominator = np.sqrt(sum([x**2 for x in v1])) \\\n",
    "                * np.sqrt(sum([x**2 for x in v2]))\n",
    "    if denominator == 0:\n",
    "        print('division by zero with words:')\n",
    "        print(n1)\n",
    "        print(n2)\n",
    "    return (numerator/denominator)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 79,
=======
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 1.25 seconds to read in dataset bow5.words_small\n"
     ]
    }
   ],
   "source": [
    "# switch between 'bow5.words', 'bow2.words' and 'deps.words'\n",
    "def get_embeddings(filename):\n",
    "    # Returns a dictionary containing words as keys and word vectors\n",
    "    #  as values\n",
    "    vectors = {}\n",
    "    f = open(filename,'r')\n",
    "    word_embeddings = defaultdict(list)\n",
    "    s = time.time()\n",
    "    for line in f:\n",
    "        entry = [x.strip() for x in line.split(' ')]\n",
    "        word = entry[0]\n",
    "        vector = entry[1:]\n",
    "        \n",
    "        word_embeddings[word] = [np.float(x) for x in vector]\n",
    "    f.close()\n",
    "    e = time.time()\n",
    "    print('It took {} seconds to read in dataset {}'.format(e-s,filename))\n",
    "    return word_embeddings\n",
    "\n",
    "bow5_embeddings = get_embeddings('bow5.words_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = {}\n",
<<<<<<< HEAD
    "f = open('bow5.words','r')\n",
    "word_embeddings = defaultdict(list)\n",
    "s = time.time()\n",
    "for line in f:\n",
    "    entry = [x.strip() for x in line.split(' ')]\n",
    "    word = entry[0]\n",
    "    vector = entry[1:]\n",
    "\n",
    "    word_embeddings[word] = [np.float(x) for x in vector]"
=======
    "with open('bow5.words_small') as f:\n",
    "    for line in f:\n",
    "        word, vector = line.split(\" \",1)\n",
    "        v = np.fromstring(vector, sep=' ', dtype='float32')\n",
    "        vectors[word] = v / np.linalg.norm(v)\n",
    "    "
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 143,
=======
   "execution_count": 52,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "import heapq\n",
    "\n",
    "def most_similar(v, N=1, *ignore):\n",
    "    similar = []\n",
    "    for word, u in word_embeddings.items():\n",
    "        if word in ignore: continue\n",
    "#         similarity = u.dot(v)\n",
    "        similarity = cosine_similarity(u, v, None, None)\n",
    "        if len(similar) < N:\n",
    "            heapq.heappush(similar, (similarity, word))\n",
    "        else:\n",
    "            heapq.heappushpop(similar, (similarity, word))\n",
    "    return sorted(similar, reverse=True)"
=======
    "analogy_file = \"questions-words.txt\""
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read google analogy test set\n",
    "test = []\n",
    "with open('questions-words.txt') as f:\n",
    "    for line in f:\n",
    "        analogy = line.strip().lower().split()\n",
    "        if(analogy[0] is not \":\"):\n",
    "            test.append(analogy)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sorted_by_similarity(words, base_vector):\n",
    "    \"\"\"Returns words sorted by cosine distance to a given vector, most similar first\"\"\"\n",
    "    words_with_distance = [(cosine_similarity(base_vector, w.vector, None, None), w) for w in words]\n",
    "    # We want cosine similarity to be as large as possible (close to 1)\n",
    "    return sorted(words_with_distance, key=lambda t: t[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def closest_analogies(a0, a1, b0, words) :\n",
    "    word_a1 = find_word(a1, words)\n",
    "    word_a0 = find_word(a0, words)\n",
    "    word_b0 = find_word(b0, words)\n",
    "    vector = add_vectors(\n",
    "        sub_vectors(word_a1.vector, word_a0.vector),\n",
    "        word_b0.vector)\n",
    "    closest = sorted_by_similarity(words, vector)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_redundant(word):\n",
    "    return (\n",
    "        a1.lower() in word.lower() or\n",
    "        a0.lower() in word.lower() or\n",
    "        b0.lower() in word.lower())\n",
    "    return [(dist, w) for (dist, w) in closest if not is_redundant(w.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_analogy(a0, a1, b0, words):\n",
    "    analogies = closest_analogies(a0, a1, b0, words)\n",
    "    if (len(analogies) == 0):\n",
    "        print(\"{a0}-{a1} is like {b0}-?\")\n",
    "    else:\n",
    "        (dist, w) = analogies[0]\n",
    "        print(\"{a0}-{a1} is like {b0}-{w.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-2573cd14628d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_analogy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Paris'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'France'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Rome'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "print_analogy('Paris', 'France', 'Rome', words)"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "test_array = np.array([np.array(i) for i in test])"
=======
    "def compare(b1_estimate, embeddings):\n",
    "    min_cos = 1000000\n",
    "    best_word = None\n",
    "    for word in list(embeddings.keys()):\n",
    "        word_vector = embeddings[word]\n",
    "        cos_sim = cosine_similarity(b1_estimate, word_vector, None, None)\n",
    "        if cos_sim < min_cos:\n",
    "            min_cos = cos_sim\n",
    "            best_word = word\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bow2_embeddings = get_embeddings('bow2.words')\n",
    "# deps_embeddings = get_embeddings('deps.words')\n"
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": null,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "a0 = [column[0] for column in test_array]\n",
    "a1 = [column[1] for column in test_array]\n",
    "b0 = [column[2] for column in test_array]"
=======
    "import heapq\n",
    "from heapq import heappush, heappushpop\n",
    "from scipy.spatial.distance import cosine\n"
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read google analogy test set\n",
    "test = []\n",
    "with open('questions-words.txt') as f:\n",
    "    for line in f:\n",
    "        analogy = line.strip().lower().split()\n",
    "        if(analogy[0] is not \":\"):\n",
    "            test.append(analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "analogy_file = np.array((a0, a1, b0))"
=======
    "test_array = np.array([np.array(i) for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19544L, 4L)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(test_array).shape"
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 92,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def offset_vector(a0, a1, b0):\n",
    "    v= a1 - a0 \n",
    "    bi_estimate = b0 + v\n",
    "    return bi_estimate"
=======
    "a0 = [column[0] for column in test_array]\n",
    "a1 = [column[1] for column in test_array]\n",
    "b0 = [column[2] for column in test_array]\n",
    "b1 = [column[3] for column in test_array]"
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
=======
   "execution_count": 93,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def get_analogy(a0, a1, b0, word_embeddings):\n",
    "    \"\"\"Returns words sorted by cosine distance to a given vector, most similar first\"\"\"\n",
    "    words_with_distance = [(cosine_similarity(word_embeddings[vector], offset_vector(analogy_file[0], analogy_file[1], analogy_file[2]), None, None), w) for w in word_embeddings[word]]\n",
    "    return sorted(words_with_distance, key=lambda t: t[0], reverse=True)"
=======
    "def offset_vector(a0, a1, b0, b1):\n",
    "    bi_estimate = (a1 - a0) + b0\n",
    "    return bi_estimate\n",
    "#     return cosine(b1, a1 - a0 + b0)\n",
    " #do it with word embeddings not words and normalize the vectors it scikit normalize l=2"
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_word(v, word_embeddings):\n",
    "    return next(w for w in word_embeddings if v == word_embeddings.keys())"
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_nearest(vectors, distance, N = 10):\n",
    "#     nearest = []\n",
    "#     for word,u in vectors.items():\n",
    "#         nearness = -distance(u)\n",
    "#         if len(nearest) < N:\n",
    "#             heapq.heappush(nearest, (nearness, word))\n",
    "#         else:\n",
    "#             heapq.heappushpop(nearest, (nearness, word))\n",
    "#     return sorted(nearest, reverse=True)"
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def closest_analogies(a0, a1, b0, word_embeddings) :\n",
    "    word_a1 = find_word(a1, word_embeddings)\n",
    "    word_a0 = find_word(a0, word_embeddings)\n",
    "    word_b0 = find_word(b0, word_embeddings)\n",
    "    vector = add_vectors(\n",
    "        sub_vectors(word_a1.vector, word_a0.vector),\n",
    "        word_b0.vector)\n",
    "    closest = get_analogy(word_embeddings, vector)[:10]"
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def print_nearest(vectors, word):\n",
    "#     v = vectors[word]\n",
    "#     print(word, np.linalg.norm(v))\n",
    "#     for nearness, word in get_nearest(vectors, lambda u: cosine(u, v)):\n",
    "#         print(word, nearness)\n",
    "        "
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 89,
=======
   "execution_count": null,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def print_analogy(a0, a1, b0, word_embeddings):\n",
    "    analogies = closest_analogies(a0, a1, b0, word_embeddings)\n",
    "    if (len(analogies) == 0):\n",
    "        print(\"{a0}-{a1} is like {b0}-?\")\n",
    "    else:\n",
    "        (dist, w) = analogies[0]\n",
    "        print(\"{a0}-{a1} is like {b0}-{w.text}\")"
=======
    "# def get_analogy(vectors, a0, a1, b0, distance):\n",
    "#     return get_nearest(vectors, lambda b1: distance(a0, a1, b0))\n"
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 90,
=======
   "execution_count": null,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def compare(b1_estimate, embeddings):\n",
    "    min_cos = 1000000\n",
    "    best_word = None\n",
    "    for word in list(embeddings.keys()):\n",
    "        word_vector = embeddings[word]\n",
    "        cos_sim = cosine_similarity(b1_estimate, word_vector, None, None)\n",
    "        if cos_sim < min_cos:\n",
    "            min_cos = cos_sim\n",
    "            best_word = word\n",
    "        \n",
    "    return best_word"
=======
    "# def print_analogy(vectors, a0, a1, b0, distance=offset_vector):\n",
    "# #     print(a0, a1, b0)\n",
    "#     for nearness, word in get_analogy(vectors, vectors[a0], vectors[a1], vectors[b0],\n",
    "#                                     distance):\n",
    "#         print(word, nearness)"
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 91,
=======
   "execution_count": null,
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
<<<<<<< HEAD
    "    vectors = word_embeddings\n",
    "    print_analogy(a0, a1, b0, word_embeddings)"
=======
    "    vectors = bow5_embeddings\n",
    "    print_analogy(vectors, a0, a1, b0, distance=offset_vector)"
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-2436fc2ab63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-a0e01f762060>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint_analogy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-aa476d8448ef>\u001b[0m in \u001b[0;36mprint_analogy\u001b[0;34m(a0, a1, b0, word_embeddings)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprint_analogy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0manalogies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosest_analogies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalogies\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{a0}-{a1} is like {b0}-?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-0a70e76d4e32>\u001b[0m in \u001b[0;36mclosest_analogies\u001b[0;34m(a0, a1, b0, word_embeddings)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclosest_analogies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mword_a1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mword_a0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword_b0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     vector = add_vectors(\n",
      "\u001b[0;32m<ipython-input-87-a04758bb3dad>\u001b[0m in \u001b[0;36mfind_word\u001b[0;34m(v, word_embeddings)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_embeddings\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> 5732751fe1f31fd9390310fc9b22763a391b11d6
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
